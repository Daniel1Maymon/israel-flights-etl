services:

  # zookeeper:
  #   container_name: zookeeper
  #   image: bitnami/zookeeper:3.9.2
  #   environment:
  #     - ALLOW_ANONYMOUS_LOGIN=yes
  #   ports:
  #     - "2181:2181"

  # course-kafka:
  #   container_name: course-kafka
  #   image: bitnami/kafka:3.7.0
  #   environment:
  #     - KAFKA_CFG_ZOOKEEPER_CONNECT=zookeeper:2181
  #     - ALLOW_PLAINTEXT_LISTENER=yes
  #     - KAFKA_CFG_LISTENERS=PLAINTEXT://:9092
  #     - KAFKA_CFG_ADVERTISED_LISTENERS=PLAINTEXT://course-kafka:9092
  #     - KAFKA_CFG_OFFSETS_TOPIC_REPLICATION_FACTOR=1
  #   ports:
  #     - "9092:9092"
  #   depends_on:
  #     - zookeeper

  # kafdrop:
  #   container_name: kafdrop
  #   image: obsidiandynamics/kafdrop:3.31.0
  #   ports:
  #     - "9003:9000"
  #   environment:
  #     - KAFKA_BROKERCONNECT=course-kafka:9092
  #   depends_on:
  #     - course-kafka

  # python-app:
  #   image: python:3.11-slim
  #   container_name: python-kafka-container
  #   command: tail -f /dev/null
  #   volumes:
  #     - ./app:/app
  #   working_dir: /app
  #   tty: true
  #   stdin_open: true
  #   entrypoint: >
  #     sh -c "apt-get update &&
  #            apt-get install -y vim nano &&
  #            pip install --no-cache-dir -r requirements.txt &&
  #            tail -f /dev/null"

  # mongo:
  #   image: mongo:8.0
  #   container_name: mongo
  #   volumes:
  #     - ./data:/de_data
  #   working_dir: /de_data
  #   ports:
  #     - "27017:27017"

  # elasticsearch:
  #   container_name: elasticsearch
  #   image: docker.elastic.co/elasticsearch/elasticsearch:8.13.4-arm64
  #   ports:
  #     - "9200:9200"
  #   environment:
  #     - discovery.type=single-node

  # kibana:
  #   container_name: kibana
  #   image: docker.elastic.co/kibana/kibana:8.13.4-arm64
  #   ports:
  #     - "5601:5601"
  #   depends_on:
  #     - elasticsearch

  postgres:
    container_name: postgres_airflow
    image: postgres:16.3
    environment:
      - POSTGRES_USER=postgres
      - POSTGRES_PASSWORD=postgres
      - POSTGRES_DB=airflow
      - POSTGRES_PORT=5432
    ports:
      - "5432:5432"
    volumes:
      - postgres_data:/var/lib/postgresql/data

  postgres_flights:
    container_name: postgres_flights
    image: postgres:16.3
    environment:
      - POSTGRES_USER=daniel
      - POSTGRES_PASSWORD=daniel
      - POSTGRES_DB=flights_db
      - POSTGRES_PORT=5432
    ports:
      - "5433:5432"
    volumes:
      - israel-flights-etl_postgres_flights_data:/var/lib/postgresql/data

  airflow-init:
    build:
      context: .
      dockerfile: Dockerfile.airflow
    image: airflow-init
    container_name: airflow_init
    depends_on:
      - postgres
    env_file:
      - ../.env
    environment:
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://postgres:postgres@postgres:5432/airflow
      - AIRFLOW__CORE__FERNET_KEY=FB0o_zt4e3Ziq3LdUUO7F2Z95cvFFx16hU8jTeR1ASM=
      - AIRFLOW__CORE__LOAD_EXAMPLES=False
      - AIRFLOW__LOGGING__LOGGING_LEVEL=INFO
      - PYTHONPATH=/opt/airflow
      # AWS S3 Connection
      - AIRFLOW__CONN__AWS_S3=aws://${AWS_ACCESS_KEY_ID}:${AWS_SECRET_ACCESS_KEY}@s3.${AWS_DEFAULT_REGION}.amazonaws.com
      # PostgreSQL Connection for Airflow metadata
      - AIRFLOW__CONN__POSTGRES_AIRFLOW=postgres://postgres:postgres@postgres:5432/airflow
      # PostgreSQL Connection for flights data
      - AIRFLOW__CONN__POSTGRES_FLIGHTS=postgres://${POSTGRES_FLIGHTS_USER}:${POSTGRES_FLIGHTS_PASSWORD}@postgres_flights:5432/flights_db
    volumes:
      - ./dags:/opt/airflow/dags
      - ./etl:/opt/airflow/etl
      - ./utils:/opt/airflow/utils
      - ./airflow-data/logs:/opt/airflow/logs
      - ./airflow-data/plugins:/opt/airflow/plugins
      - ./data:/opt/airflow/data
      - ./config:/opt/airflow/config
    entrypoint: /bin/bash
    command:
      - -c
      - |
        until pg_isready -h postgres -p 5432 -U postgres; do
          echo "Waiting for Postgres..."
          sleep 2
        done
        echo "Postgres is ready."

        airflow db migrate

        airflow users list | grep -q "airflow" || \
        airflow users create \
          --role Admin \
          --username airflow \
          --password airflow \
          --email airflow@airflow.com \
          --firstname airflow \
          --lastname airflow

        echo "Airflow initialization completed."
    restart: "no"

  airflow-webserver:
    build:
      context: .
      dockerfile: Dockerfile.airflow
    depends_on:
      airflow-init:
        condition: service_completed_successfully
    env_file:
      - ../.env
    environment:
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://postgres:postgres@postgres:5432/airflow
      - AIRFLOW__CORE__FERNET_KEY=FB0o_zt4e3Ziq3LdUUO7F2Z95cvFFx16hU8jTeR1ASM=
      - AIRFLOW__CORE__LOAD_EXAMPLES=True
      - AIRFLOW__LOGGING__LOGGING_LEVEL=INFO
      - PYTHONPATH=/opt/airflow
    volumes:
      - ./dags:/opt/airflow/dags
      - ./etl:/opt/airflow/etl
      - ./utils:/opt/airflow/utils
      - ./airflow-data/logs:/opt/airflow/logs
      - ./airflow-data/plugins:/opt/airflow/plugins
      - ./data:/opt/airflow/data
      - ./config:/opt/airflow/config
    ports:
      - 8082:8080
    command: airflow webserver
    restart: always

  airflow-scheduler:
    build:
      context: .
      dockerfile: Dockerfile.airflow
    depends_on:
      airflow-init:
        condition: service_completed_successfully
    env_file:
      - ../.env
    environment:
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://postgres:postgres@postgres:5432/airflow
      - AIRFLOW__CORE__FERNET_KEY=FB0o_zt4e3Ziq3LdUUO7F2Z95cvFFx16hU8jTeR1ASM=
      - AIRFLOW__CORE__LOAD_EXAMPLES=False
      - AIRFLOW__LOGGING__LOGGING_LEVEL=INFO
      - PYTHONPATH=/opt/airflow
      # AWS S3 Connection
      - AIRFLOW__CONN__AWS_S3=aws://${AWS_ACCESS_KEY_ID}:${AWS_SECRET_ACCESS_KEY}@s3.${AWS_DEFAULT_REGION}.amazonaws.com
      # PostgreSQL Connection for Airflow metadata
      - AIRFLOW__CONN__POSTGRES_AIRFLOW=postgres://postgres:postgres@postgres:5432/airflow
      # PostgreSQL Connection for flights data
      - AIRFLOW__CONN__POSTGRES_FLIGHTS=postgres://${POSTGRES_FLIGHTS_USER}:${POSTGRES_FLIGHTS_PASSWORD}@postgres_flights:5432/flights_db
    volumes:
      - ./dags:/opt/airflow/dags
      - ./etl:/opt/airflow/etl
      - ./utils:/opt/airflow/utils
      - ./airflow-data/logs:/opt/airflow/logs
      - ./airflow-data/plugins:/opt/airflow/plugins
      - ./data:/opt/airflow/data
      - ./config:/opt/airflow/config
    command: airflow scheduler
    restart: always

volumes:
  postgres_data:
  postgres_flights_data:
  israel-flights-etl_postgres_flights_data:
    external: true
